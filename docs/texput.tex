%%% Local Variables:
%%% mode: latex
%%% TeX-master: "<none>"
%%% End:

\title{Motion Compensated Discrete Wavelet Transform (MCDWT)}

\author{Vicente Gonz√°lez Ruiz}

\maketitle
\tableofcontents

\section{Intro}

\href{https://en.wikipedia.org/wiki/Video}{Video} data contain high
amounts of redundancy, spatial and temporal. For this reason, most of
video encoders compress the input sequece of
\href{https://en.wikipedia.org/wiki/Digital_image}{images} (which are
matrices of \href{https://en.wikipedia.org/wiki/Pixel}{pixels})
basically in two stages: (1) a transform stage in the
\href{https://en.wikipedia.org/wiki/Time_domain}{time} and in the
\href{https://www.quora.com/What-is-spatial-domain-in-image-processing}{spatial}
domains that produces a collection of spatially
\href{https://en.wikipedia.org/wiki/Decorrelation}{uncorrelated}
\href{https://en.wikipedia.org/wiki/Discrete_wavelet_transform}{coefficients},
and (2) an
\href{https://vicente-gonzalez-ruiz.github.io/symbol_compression/}{entropy
  encoding} phase which removes the statistical redundancy that can
still remains after the decorrelation (transform). These coefficients
have two interesting features:
\begin{enumerate}
\item Usually, a \textbf{smaller
  \href{https://en.wikipedia.org/wiki/Discrete_wavelet_transform}{entropy}}
  than the original pixels. This helps to increase the
  \href{https://en.wikipedia.org/wiki/Data_compression_ratio}{compression
    ratio}.
\item The transform domain provides a
  \textbf{\href{https://en.wikipedia.org/wiki/Image_resolution}{multiresolution}
    representation (spatial and temporal)} of the visual information (spatial
  \href{http://inst.eecs.berkeley.edu/~ee290t/sp04/lectures/videowavelet_UCB1-3.pdf}{scalability}).
\end{enumerate}

\section{Distrete Wavelet Transform (DWT)}
\href{https://en.wikipedia.org/wiki/Discrete_wavelet_transform}{(2D-)DWT}
is a digital transform that, applied to an image, performs spatial
decorrelation (2D) and obtains a
\href{https://vicente-gonzalez-ruiz.github.io/image_transformations_for_coding/index.html#x1-3500024}{multiresolution
  dyadic representation} of such image, conforming a collection of DWT
subbands. The forward transform converts the image into a set of
frequency subbands with coefficients representing different spatial
areas and frequency orientations. There is an equivalence between
(DWT) subbands (a \emph{(subband) decomposition}) and
\href{http://fourier.eng.hmc.edu/e161/lectures/canny/node3.html}{Laplacian
  pyramids}, and it is quite simple to
\href{https://vicente-gonzalez-ruiz.github.io/pyramids-and-wavelets/}{pass
  from a decomposition representation to a pyramid representation and
  viceversa}.

\href{https://github.com/vicente-gonzalez-ruiz/MCDWT/blob/master/src/DWT.py}{DWT.py}
implements the forward and backward (2D-)DWT for color images. $L$ and
$H$ stands for \emph{low-pass filtered} and \emph{high-pass filtered},
respectively.  More information about the implementation can be found at
\href{https://pywavelets.readthedocs.io/en/latest/index.html}{PyWavelets}.

\lstinputlisting[firstline=9, lastline=105, language=python, caption=DWT.py]{../src/DWT.py}

The multilevel DWT is a recursive use of the (1-level) DWT applied to
the $LL$ subband. For example, if the DWT is applied $k$-times to the
$LL$ subband, recursively, a decomposition of $k+1$-levels is generated. For
the sake of simplicity, we will denote the subbands $\{LH^k, HL^k,
HH^k\}$ as only $H^k$, and $LL^k$ as only $L^k$.

\subsection{Scalability provided by DWT}
Depending on the number of subbands levels (pyramid levels in the
pyramid representation) that we use, we can reconstruct a image at
different spatial scales (resolutions). For example, if an image $s_1$ has been transformed using the DWT of 2 levels, we get:
\begin{itemize}
\item Subband $s_1.L^2$ with the scale 2.
\item Subband $s_1.L^1=s_1.L=\text{iDWT}(s_1.L^2, S_1.H^2)$ (iDWT =
  inverse DWT = backward DWT) with the scale 1.
\item $s_1.L^0=\text{iDWT}(s_1.L, s_1.H)$ (the original image) with the scale 0.
\end{itemize}

\section{Motion DWT (MDWT)}
DWT can be applied to a sequence of images by simply transforming each
image of the sequence independently. This is done, for example, in the
\href{https://en.wikipedia.org/wiki/JPEG_2000}{motion JPEG2000 video
  compression standard}. Notice that only the spatial redundancy is
exploited. All the temporal redundancy still remains in the video.

\begin{figure}
\centering
\imgw{800}{graphics/forward_MDWT.svg}
\caption{Forward MDWT step.}
\end{figure}

\lstinputlisting[firstline=16, lastline=62, language=python, caption=MDWT.py]{../src/MDWT.py}

\subsection{Scalability provided by MDWT}
MDWT sequences (of (subband) decompositions) are scalable in space and
in time. Spatial scalability is a direct consequence of DWT. On the other
hand, it is trivial to observe that MDWT provides \emph{fully}
temporal scalability (we can access to the images randomly) because
each image of the input sequence is transformed independently.

\section{Video transform alternatives}
To remove both, spatial and temporal redundancies, two different
alternatives are available: (1) t+2D transforms and (2) 2D+t
transforms. In a t+2D transform, the video is first
\href{https://en.wikipedia.org/wiki/Digital_filter\#Analysis_techniques}{analyzed}
(transformed) over the time domain and next, over the space domain. A
2D+t transform does just the opposite. Each choice has a number of
\emph{pros} and \emph{cons}. For example, in a t+2D transform we can
apply directly any image predictor based on
\href{https://en.wikipedia.org/wiki/Motion_estimation}{motion
  estimation} because the input is a normal video. However, if we
implement a 2D+t transform, the input to the motion estimator is a
sequence decompositions.
\href{http://www.polyvalens.com/blog/wavelets/theory}{The overwhelming
  majority of DWT's} are not
\href{http://www.polyvalens.com/blog/wavelets/theory}{shift
  invariant}, which basically means $\text{DWT}(s[t]) \neq
\text{DWT}(s[t+x])$, where $x$ is a displacement of $s[t]$ along one of the
signal domains.  Therefore, motion estimators which compare pixel values
will not work on the (subband) decomposition domain. On the other
hand, if we want to provide true spatial scalability (processing only
those
\href{https://en.wikipedia.org/wiki/Image_resolution\#Spatial_resolution}{spatial
  resolutions} (scales) necessary to get a spatially scaled of our
video), a t+2D transformed video is not suitable because the first
step of the forward transform (t) should be reversed at full
resolution in the backward transform (as the forward transform did).

\section{Motion Compensated Discrete Wavelet Transform (MCDWT)}
MCDWT is a 2D+t transform. The 2D stage is MDWT. The t stage is a
1D-DWT, which removes the temporal redundancy between adjacent $H$
subbands by means of
\href{https://en.wikipedia.org/wiki/Motion_compensation}{Motion
  Compensation (MC)}. A special characteristic of MCDWT is that the
motion information is not transmitted from the encoder to the
\href{https://en.wikipedia.org/wiki/Decoder}{decoder}. This is possible because to estimate de motion at the
encoder, only the information that it is accesible by the
decoder is used.

\subsection{MCDWT butterfly}

MCDWT butterly inputs three decompositions $a=\{a.L, a.H\}$, $b=\{b.L,
b.H\}$ and $c=\{c.L, c.H\}$, and outputs a residue subband
$\tilde{b}.H$, which replaces to $b.H$ in the original $b$
decomposition. So, after the use of the bufferfly, we get $a=\{a.L,
a.H\}$ (an intra-coded ``I'' image), $\tilde{b}=\{b.L, \tilde{b}.H\}$
(a bidirectionally predicted ``B'' image) and $c=\{c.L, c.H\}$ (an
intra-coded ``I'' image). Obviously, this replacement is fully
reversible. Notice that the
\href{http://www.vtvt.ece.vt.edu/research/references/video/DCT_Video_Compression/Zhang92a.pdf}{pyramid
  domain} (which is invariant to the pixels displacements) has been
used to estimate and compensate the $H$ subbands.

\begin{figure}
\centering
\imgw{1200}{graphics/forward_butterfly.svg}
\caption{Forward MCDWT butterfly.}
\end{figure}
\lstinputlisting[firstline=20, lastline=67, language=python, caption=MCDWT.py]{../src/MCDWT.py}

Notice that, even if $a$ and $c$ (at full resolution) are available at
the decoder, only $b.L$ is. So, it does not make sense to search the
low resolution structures of $b.L$ in the high resolution images $a$
and $c$, because even if a perfect match is achieved (think about the
tree images $a$, $b$ and $c$ are identical), the prediction error
would be different than $0$ as a consequence of that the high
resolution information is missing in $b.L$.

\subsection{MCDWT forward transform}

The MCDWT forward transform is the result of appliying the MCDWT
butterfly to all the input sequence of decompositions with different
distances between them. As a consequence, except the first and the
last $H$-subband of the sequence are temporally decorrelated.

\begin{figure}
\centering
\imgw{1200}{graphics/temporal_decorrelation.svg}
\caption{MCDWT forward transform.\label{fig:forward_MCDWT}}
\end{figure}
\lstinputlisting[firstline=69, lastline=138, language=python, caption=MCDWT.py]{../src/MCDWT.py}

\subsection{(Spatial) Multiresolution procedure}

Spatial dyadic multiresolution can be obtained by appliying a sequence of MDWT+MCDWT steps (see Fig.~\ref{fig:multiresolution}).

\begin{figure}
\centering
\imgw{1200}{graphics/multiresolution.svg}
\caption{MDWT+MCDWT multiresolution procedure.\label{fig:multiresolution}}
\end{figure}


\subsection{Scalability provided by MCDWT}
The decomposition generated by the MCDWT is identical to
MDWT. Therefore, MCDWT provides the same spatial scalability than
MDWT. In order to obtain several spatial resolutions, MCDWT should be
applied to the low-frequency subbands, recursively. As an example, in
the Figure~\ref{fig:multiresolution}, MCDWT has been applied to the
output of the example of the Figure~\ref{fig:forward_MCDWT}. Notice
that between two consecutive iterations of MCDWT, MDWT must be applied
to the low-frequency subband in order to reduce the resolution of the
analysis.

On the other hand, MCDWT decreases the temporal scalability offered by
MDWT to allowing only dyadic access (depending on required image and
the number of MCDWT levels, more than one decomposition need to be
inversely transformed). For example, if only one step of the backward
transform is applied to the previous example, only the decompositions
of $s_0$, $s_2$ and $s_4$ will be reconstructed (the first
\href{https://en.wikipedia.org/wiki/Temporal_resolution}{temporal
  resolution}. If a second iteration of the backward transform is
used, all the decompositions are rendered.

\begin{comment}

\section{Video \href{http://inst.eecs.berkeley.edu/~ee290t/sp04/lectures/videowavelet_UCB1-3.pdf}{scalability}}
MCDWT inputs a \href{https://en.wikipedia.org/wiki/Video}{video} (a
sequence of images) and outputs a video (a sequece of pyramids), in
such a way that when only a portion of the data of the transformed
video is used, a video with a lower
\href{https://en.wikipedia.org/wiki/Temporal_resolution}{temporal
resolution}, lower
\href{https://en.wikipedia.org/wiki/Image_resolution\#Spatial_resolution}{spatial
resolution} or/and lower
\href{https://en.wikipedia.org/wiki/Compression_artifact}{quality} can
be generated.

If all the transformed data is used, then the original video is
restored (MCDWT is a lossless transform). The forward transform's
output has exactly the same number of pyramids than images are in the
input video (for example, no extra motion fields are produced). At
this time, we will focuse only on spatial
\href{http://inst.eecs.berkeley.edu/~ee290t/sp04/lectures/videowavelet_UCB1-3.pdf}{scalability}. Window
Of Interest (WOI) and quality scabilities will be addressed later.

\end{comment}
