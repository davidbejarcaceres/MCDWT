{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion Compensated Discrete Wavelet Transform (MCDWT)\n",
    "\n",
    "MCDWT is a multiresolution [video] transform. The input sequence of [pixels] are [decorrelated] in the [time] and in the [spatial] domains. The output sequence of [coefficients] have a smaller [entropy] than the original pixels, and the information is represented by temporal and spatial [resolution levels].\n",
    "\n",
    "* In MCDWT (and in general, in video coding) temporal decorrelation is provided by [MC (Motion Compensation)], where the [prediction images] are generated with an algorithm that uses only the information available at the [decoder]. \n",
    "\n",
    "* Spatial decorrelation is performed by the [analysis filtering] used in the [2D-DWT].\n",
    "\n",
    "[video]: https://en.wikipedia.org/wiki/Video\n",
    "[decorrelator]: https://en.wikipedia.org/wiki/Decorrelation\n",
    "[visual]: https://en.wikipedia.org/wiki/Visual_perception\n",
    "[pixels]: https://en.wikipedia.org/wiki/Pixel\n",
    "[decorrelated]: https://en.wikipedia.org/wiki/Decorrelation\n",
    "[time]: https://en.wikipedia.org/wiki/Time_domain\n",
    "[spatial]: https://www.quora.com/What-is-spatial-domain-in-image-processing\n",
    "[coefficients]: https://www.quora.com/What-is-spatial-domain-in-image-processing\n",
    "[entropy]: https://en.wikipedia.org/wiki/Entropy\n",
    "[resolution levels]: https://en.wikipedia.org/wiki/Image_resolution\n",
    "[MC (Motion Compensation)]: https://en.wikipedia.org/wiki/Motion_compensation\n",
    "[decoder]: https://en.wikipedia.org/wiki/Decoder\n",
    "[prediction images]: https://en.wikipedia.org/wiki/Decoder\n",
    "[analysis filtering]: https://en.wikipedia.org/wiki/Digital_filter#Analysis_techniques\n",
    "[2D-DWT]: https://en.wikipedia.org/wiki/Discrete_wavelet_transform\n",
    "[EBCOT]: http://nptel.ac.in/courses/117105083/pdf/ssg_m5l15.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Video [scalability]\n",
    "\n",
    "MCDWT inputs a [video] and outputs a video, in a way that when only a portion of the data of the transformed video is used, a video with a lower [temporal resolution], lower [spatial resolution] or/and lower\n",
    "[quality] can be generated.\n",
    "\n",
    "If all the transformed data is used, then the original video is restored (MCDWT is a lossless transform). The forward transform's output has exactly the same number of elements than the input video (for example, no extra motion fields are produced). At this time, we will focuse only on spatial [scalability]. Other types of scabilities will be addressed later.\n",
    "\n",
    "[temporal resolution]: https://en.wikipedia.org/wiki/Temporal_resolution\n",
    "[spatial resolution]: https://en.wikipedia.org/wiki/Image_resolution#Spatial_resolution\n",
    "[quality]: https://en.wikipedia.org/wiki/Compression_artifact\n",
    "[scalability]: http://inst.eecs.berkeley.edu/~ee290t/sp04/lectures/videowavelet_UCB1-3.pdf\n",
    "[video]: https://en.wikipedia.org/wiki/Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Video transformation alternatives\n",
    "\n",
    "To obtain a multiresolution version or a video, the [DWT] (Discrete\n",
    "Wavelet Transform) can be applied along temporal (t) and\n",
    "spatial domains (2D). At this point, two different alternatives\n",
    "are available: (1) a t+2D transform or (2) a 2D+t\n",
    "transform. In a t+2D transform, the video is first analyzed\n",
    "over the time domain and next, over the space domain. A 2D+t\n",
    "transform does just the opposite.\n",
    "\n",
    "[DWT]: https://en.wikipedia.org/wiki/Discrete_wavelet_transform\n",
    "\n",
    "Each choice has a number of *pros* and *cons*. For example, in a\n",
    "t+2D transform we can apply directly any image predictor based\n",
    "on motion estimation because the input is a normal video. However, if\n",
    "we implement a 2D+t transform, the input to the motion estimator is a sequence of images in the DWT domain. [The overwhelming majority of DWT's][Friendly Guide] are not [shift invariant], which basically means $\\text{DWT}(s[t]) \\neq \\text{DWT}(s[t+x])$, where $x$ is a\n",
    "displacement of $s[t]$ along the time domain. Therefore, motion estimators which compare pixel values will not work on the DWT domain. On the other hand, if we want to provide true spatial scalability (processing only those spatial resolutions (scales) necessary to get a spatially scaled of our video), a t+2D transformed video is not suitable because the first step of the forward transform (t) should be reversed at full resolution in the backward transform (as the forward transform did).\n",
    "\n",
    "[shift invariant]: http://www.polyvalens.com/blog/wavelets/theory\n",
    "[Friendly Guide]: http://www.polyvalens.com/blog/wavelets/theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Wavelet and pyramid domains\n",
    "\n",
    "Indeed, the DWT allows to get a scalable representation of a image, and by extension of a video, when we apply the DWT on all the images of the video. However, this can be also done with [Gaussian and Laplacian pyramids][Laplacian Pyramids]. Image pyramids are interesting because they are shift invariant and therefore, one can operate within the scales as they are *normal* images. Unfortunately, as a consecuence of pyramids representations are not critically sampled, they need more picture elements than in [wavelet pyramids][Wavelet Pyramids], and this is a serious drawback when compressing. Luckily, it is very fast to convert a Laplacian pyramid representation into it DWT equivalent representation, and viceversa. For this reason, even if we use wavelet pyramids in our images, we can suppose at any moment that we are working with the Laplacian pyramid of those images.\n",
    "\n",
    "[Laplacian Pyramids]: https://en.wikipedia.org/wiki/Pyramid_(image_processing)\n",
    "[Wavelet Pyramids]: http://www.vtvt.ece.vt.edu/research/references/video/DCT_Video_Compression/Zhang92a.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The \\texttt{K}-levels 2D Discrete Wavelet Transform (2D-DWT) of a video\n",
    "\n",
    "As said before, a [2D-DWT][2D-DWT] (2 Dimensions - Discrete Wavelet Transform) generates a scalable representation of an image and by extension, of a video if we apply the DWT to all the images of the video. This is done, for example, in the [motion JPEG2000 video compression standard][J2K]. Notice that only the spatial redundancy is exploited. All the temporal redundancy is still in the video.\n",
    "\n",
    "[J2K]: https://en.wikipedia.org/wiki/JPEG_2000\n",
    "[2D-DWT]: https://en.wikipedia.org/wiki/Discrete_wavelet_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "\n",
    "A sequence $s$ of $N$ images:\n",
    "\n",
    "```\n",
    "                                                      x \n",
    "+---------------+  +---------------+     +---------------+\n",
    "|               |  |               |     |            |  |\n",
    "|               |  |               |   y |----------- o <---- s[N-1][y][x]\n",
    "|               |  |               | ... |               |\n",
    "|               |  |               |     |               |\n",
    "|               |  |               |     |               |\n",
    "|               |  |               |     |               |\n",
    "+---------------+  +---------------+     +---------------+\n",
    "      s[0]               s[1]                 s[N-1]\n",
    "```\n",
    "\n",
    "### Output\n",
    "\n",
    "A sequence $S$ of $N$ pyramids. For example, a ($K=2$)-levels, the 2D-DWT looks like:\n",
    "\n",
    "```\n",
    "+---+---+-------+  +---+---+-------+     +---+---+-------+\n",
    "|LL2|HL2|       |  |   |   |       |     |   |   |       |\n",
    "+---+---+  HL1  |  +---+---+       |     +---+---+       |\n",
    "|LH2|HH2|       |  |   |   |       |     |   |   |       |\n",
    "+---+---+-------+  +---+---+-------+ ... +---+---+-------+\n",
    "|       |       |  |       |       |     |       |       |\n",
    "|  LH1  |  HH1  |  |       |       |     |       |       |\n",
    "|       |       |  |       |       |     |       |       |        \n",
    "+-------+-------+  +-------+-------+     +-------+-------+\n",
    "      S[0]               S[1]                  S[N-1]\n",
    "```\n",
    "where $L$ and $H$ stands for *low-pass filtered* and *high-pass filtered*, respectively. The integer > 1 that follows these letters represents the subband level. For the sake of simplicity, we will denote the subbands $\\{LH^k, HL^k, HH^k\\}$ as only $H^k$, and $LL^k$ as only $L^k$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\\lstinputlisting[language=python]{src/sequence/2D_DWT.py}\n",
    "```python\n",
    "def sequence_2D_DWT(s, K):\n",
    "    for image in s:\n",
    "        _2D_DWT(image, K) # In-place computation\n",
    "    return s\n",
    "    \n",
    "S = sequence_2D_DWT(s, K)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse algorithm\n",
    "\\lstinputlisting[language=python]{src/sequence/2D_iDWT.py}\n",
    "\n",
    "```python\n",
    "def sequence_2D_iDWT(S, K):\n",
    "    for pyramid in S:\n",
    "        _2D_iDWT(pyramid, K) # In-place computation\n",
    "    return S\n",
    "    \n",
    "s = sequence_2D_iDWT(S, K)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalability\n",
    "\n",
    "The 2D-DWT applied to a video produces a scalable representation in the space (we can extract different videos with different spatial scales or resolutions), in the time (we can extract diferent videos with different number of frames), and in quality (we can get the DWT\n",
    "coefficients with different quantization steps to reconstruct videos of different quality). In the last example, subbands $\\{S_0.LL^2, S_1.LL^2, ..., S_{N-1}.LL^2\\}$ represent the scale (number) 2 of the original video (the spatial resolution of this is the resolution of $s$ divided by 4 in each spatial dimension).\n",
    "\n",
    "For example, to reconstruct the scale 1 (which is provided by the subbands $\\{LL^2, LH^2, HL^2, HH^2\\}$), we must apply the 2D-iDWT (1-levels 2D inverse DWT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of 2D-DWT and 2D-iDWT\n",
    "\n",
    "See for example, [pywt.wavedec2()](https://pywavelets.readthedocs.io/en/latest/ref/2d-dwt-and-idwt.html#d-multilevel-decomposition-using-wavedec2) at [PyWavelets](https://pywavelets.readthedocs.io/en/latest/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Redundancy and compression\n",
    "\n",
    "The 2D-DWT incorporates to \\texttt{S} an interesting feature: usually, \\texttt{H} subbands has a lower entropy than \\texttt{s}. This means that if we apply to \\texttt{S} an entropy encoder, we can get a shorter representation of the video than if we encode \\texttt{s} directly. This is a consequence of that 2D-DWT removes (most of) the spatial redudancy of the images of the video (neighboring pixels tend to have similar values and their substraction tends to produce zeros)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Why Motion Compensated DWT (MCDWT)?\n",
    "\n",
    "As we have said, the 2D-DWT does not exploit the temporal redundancy of a video. Consequently, we can achieve higher compression ratios (in addition to the 2D-DWT) if we apply a 1D-DWT along the temporal domain. This is exactly what MCDWT does. However, due to the temporal redundancy is generated mainly by the presence of objects in the scene of the video which are moving with respect to the camera, some sort of motion compensation should be used before substacting the prediction images to the original ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Forward (direct) MCDWT (1-levels) butterfly\n",
    "\n",
    "MCDWT is based on the idea that, to provide MC without transmitting the motio information from the encoder to the decoder, the motion information is computed at both ends, using the information that is available at the decoder.\n",
    "\n",
    "### Input\n",
    "\n",
    "Three frames of $s$: $a$, $b$ and $c$.\n",
    "\n",
    "### Output\n",
    "\n",
    "Three frames $A=\\{A.L, A.H\\}$ (intra-coded), $B=\\{B.L, \\tilde{B}.H\\}$ (bidirectionally predicted) and $C=\\{C.L, C.H\\}$ (intra-coded).\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "![MCDWT](forward.png)\n",
    "\n",
    "```python\n",
    "A.L, A.H = 2D_DWT(a)\n",
    "B.L, B.H = 2D_DWT(b)\n",
    "C.L, C.H = 2D_DWT(c)\n",
    "[A.L] = 2D_iDWT(A.L,0) # Interpolate low-freqs\n",
    "[A.H] = 2D_iDWT(0,A.H) # Interpolate high-freqs\n",
    "[B.L] = 2D_iDWT(B.L,0)\n",
    "[B.H] = 2D_iDWT(0,B.H)\n",
    "[C.L] = 2D_iDWT(C.L,0)\n",
    "[C.H] = 2D_iDWT(0,C.H)\n",
    "[B.L]->[A.L] = ME([B.L], [A.L]) # Motion estimation\n",
    "[B_A.H] = MC([A.H], [B.L]->[A.L]) # Project [A.H] using [B.L]->[A.L]\n",
    "[B.L]->[C.L] = ME([B.L], [C.L])\n",
    "[B_C.H] = MC([C.H], [B.L]->[C.L])\n",
    "[~B.H] = [B.H] - [B_A.H]/2 + [B_C.H]/2\n",
    "~B.H = 2D_DWT([~B.H])\n",
    "A = A.L, A.H\n",
    "B = B.L, ~B.H\n",
    "C = C.L, C.H\n",
    "```\n",
    "\n",
    "Notice that, even if $a$ and $c$ (at full resolution) are available at the decoder, only $B.L$ is. So, does not make sense to search the low resolution structures of $B.L$ in the high resolution images $a$ and $c$, because even if a perfect match is achieved, the prediction error would be different than $0$ as a consequence of the high resolution information is missing in $B.L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Another_example'></a>\n",
    "## Another example (only butterfly)\n",
    "\n",
    "Lets suppose that we have procesed the sequence of images one step and a second one is performed on the output of this first step. At this moment, the structure of the images would be:\n",
    "\n",
    "<img src=\"graphics/after_inverse_step.svg\">\n",
    "\n",
    "### Analyze the input images\n",
    "\n",
    "\\begin{equation}\n",
    "  \\{A.L^2, A.H^2\\} = \\text{DWT}(A.L)\\\\\n",
    "  \\{B.L^2, B.H^2\\} = \\text{DWT}(B.L)\\\\\n",
    "  \\{C.L^2, C.H^2\\} = \\text{DWT}(C.L)\n",
    "\\end{equation}\n",
    "\n",
    "### Zoom-in $L$ subbands\n",
    "\n",
    "\\begin{equation}\n",
    "  [A.L^2] = \\text{iDWT}(A.L^2, 0)\\\\\n",
    "  [B.L^2] = \\text{iDWT}(B.L^2, 0)\\\\\n",
    "  [C.L^2] = \\text{iDWT}(C.L^2, 0)\n",
    "\\end{equation}\n",
    "\n",
    "### Merge and zoom-in $H$ subbands\n",
    "\n",
    "\\begin{equation}\n",
    "  [A.H^2] = \\text{iDWT}(0, A.H^2)\\\\\n",
    "  [B.H^2] = \\text{iDWT}(0, B.H^2)\\\\\n",
    "  [C.H^2] = \\text{iDWT}(0, C.H^2)\n",
    "\\end{equation}\n",
    "\n",
    "### Generate a prediction $[\\hat{B}.H^2]$¡\n",
    "\n",
    "\\begin{equation}\n",
    "  [B_A.H^2] = [A.H^2]\\overset{[A.L^2]\\rightarrow[B.L^2]}{\\longrightarrow}[B.H^2]\\\\\n",
    "  [B_C.H^2] = [C.H^2]\\overset{[C.L^2]\\rightarrow[B.L^2]}{\\longrightarrow}[B.H^2]\\\\  \n",
    "\\end{equation}\n",
    "\n",
    "Note: predictions do not need to be exhaustive. Depending on the quality of the predictions for $[B.L^2]$:\n",
    "\n",
    "\\begin{equation}\n",
    "  [B_A.L^2] = [A.L^2]\\overset{[A.L^2]\\rightarrow[B.L^2]}{\\longrightarrow}[B.L^2]\\\\\n",
    "  [B_C.L^2] = [C.L^2]\\overset{[C.L^2]\\rightarrow[B.L^2]}{\\longrightarrow}[B.L^2],\\\\  \n",
    "\\end{equation}\n",
    "\n",
    "comparing $[B_A.L^2]$ and $[B_C.L^2]$ with $[B.L^2]$. In fact, $[B_A.H^2]$ and $[B_C.H^2]$ could have \"holes\" (regions to 0, supposing that the pixels are positive), producing a parcial prediction $[\\hat{B}.H^2]$ for $[B.H^2]$. In the simplest case\n",
    "\n",
    "\\begin{equation}\n",
    "  [\\hat{B}.H^2] = \\frac{[B_A.H^2] + [B_C.H^2]}{2}.\n",
    "\\end{equation}\n",
    "\n",
    "Finally, (as in the forward transform) the prediction should take into cosideration how to minimize the energy of the residue\n",
    "\n",
    "\\begin{equation}\n",
    "  [\\tilde{B}.L^2] = [B.L^2] - [\\hat{B}.L^2].\n",
    "\\end{equation}\n",
    "\n",
    "### Compute the residue $[\\tilde{B}.H^2]$\n",
    "\n",
    "\\begin{equation}\n",
    "  [\\tilde{B}.H^2] = [B.H^2] - [\\hat{B}.H^2]\n",
    "\\end{equation}\n",
    "\n",
    "### Unmerge $\\tilde{B}.H^2$ subbands\n",
    "\n",
    "\\begin{equation}\n",
    "  \\tilde{B}.H^2 = \\text{DWT}(0, [\\tilde{B}.H^2])\n",
    "\\end{equation}\n",
    "\n",
    "### Compose the residue in the wavelet domain\n",
    "\n",
    "\\begin{equation}\n",
    "   \\{B.L^2, \\tilde{B}.H^2\\}\n",
    "\\end{equation}\n",
    "\n",
    "<img src=\"graphics/before_inverse_step.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Backward (inverse) MCDWT butterfly\n",
    "\n",
    "#### Input\n",
    "\n",
    "* Three frames $A=\\{A.L, A.H\\}$, $B=\\{B.L, \\tilde{B}.H\\}$, and $C=\\{C.L, C.H\\}$. \n",
    "\n",
    "#### Output\n",
    "\n",
    "* Three frames $a$, $b$ and $c$.\n",
    "\n",
    "#### Algorithm\n",
    "\n",
    "![MCDWT](backward.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example continuation (only one butterfly)\n",
    "\n",
    "Lets restore the structure for the three transformed images of [the previous example](#Another_example).\n",
    "\n",
    "<img src=\"graphics/before_inverse_step.svg\">\n",
    "\n",
    "### Zoom-in $L$ subbands\n",
    "\n",
    "\\begin{equation}\n",
    "  [A.L^2] = \\text{iDWT}(A.L^2, 0)\\\\\n",
    "  [B.L^2] = \\text{iDWT}(B.L^2, 0)\\\\\n",
    "  [C.L^2] = \\text{iDWT}(C.L^2, 0)\n",
    "\\end{equation}\n",
    "\n",
    "### Merge (and zoom-in) $H$ subbands\n",
    "\n",
    "\\begin{equation}\n",
    "  [A.H^2] = \\text{iDWT}(0, A.H^2)\\\\\n",
    "  [\\tilde{B}.H^2] = \\text{iDWT}(0, \\tilde{B}.H^2)\\\\\n",
    "  [C.H^2] = \\text{iDWT}(0, C.H^2)\n",
    "\\end{equation}\n",
    "\n",
    "### Generate a prediction $[\\hat{B}.H^2]$\n",
    "\n",
    "Identical to the prediction step performed at the forward transform.\n",
    "\n",
    "### Restore the original subband $[B.H^2]$\n",
    "\n",
    "\\begin{equation}\n",
    "  [B.H^2] = [\\hat{B}.H^2] + [\\tilde{B}.H^2]\n",
    "\\end{equation}\n",
    "\n",
    "### Compute $L$ subbands\n",
    "\n",
    "\\begin{equation}\n",
    "  A.L = [A.L^2] + [A.H^2]\\\\\n",
    "  B.L = [B.L^2] + [B.H^2]\\\\\n",
    "  C.L = [C.L^2] + [C.H^2]\n",
    "\\end{equation}\n",
    "\n",
    "<img src=\"graphics/after_inverse_step.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $K$-levels (forward) MCDWT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MCDWT input\n",
    "\n",
    "* A sequence $s$ of $N$ images.\n",
    "\n",
    "#### MCDWT output\n",
    "\n",
    "* A sequence of $K+1$ temporal subbands, where each subband is a sequence of $N/2^K$ (wavelet) pyramids.\n",
    "\n",
    "#### Algorithm\n",
    "\n",
    "<img src=\"mcdwt_representation.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data extraction examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal scalability\n",
    "\n",
    "* Scale 2:\n",
    "\n",
    "  $\n",
    "    s_0.L^0 = \\text{DWT}^{-2}(S_0) = \\text{DWT}^{-1}(\\text{DWT}^{-1}(s_0.L^2, s_0.H^2), s_0.H^1),\\\\\n",
    "    s_4.L^0 = \\text{DWT}^{-2}(S_4),\\\\\n",
    "    s_2.L^0 = \\text{MCDWT}^{-2}(S_0, S_2, S_4) = (s_2.L^1 = \\text{MCDWT}^{-1}(\\{s_0.L^2,s_0.H^2\\}, \\{s_4.L^2, s_4.H^2\\}, \\{s_2.L^2, \\tilde{s}_2.H^2\\})) \n",
    "  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spatial scalability\n",
    "\n",
    "* Scale 2:\n",
    "\n",
    "  $\n",
    "    s_0.L^2, s_1.L^2, s_2.L^2, s_3.L^2, s_3.L^2.\n",
    "  $\n",
    " \n",
    "* Scale 1:\n",
    "\n",
    "  $\n",
    "    s_0.L^1 = DWT^{-1}(s_0.L^2, s_0,H^2), \\\\\n",
    "    s_4.L^1 = DWT^{-1}(s_4.L^2, s_4.H^2), \\\\\n",
    "    s_2.L^1 = MCDWT^{-1}(s_0.L^1, s_2.L^2, s_4.L^1), \\\\\n",
    "    s_1.L^1 = MCDWT^{-1}(s_0.L^1, s_1.L^2, s_2.L^1), \\\\\n",
    "    s_3.L^1 = MCDWT^{-1}(s_2.L^1, s_3.L^2, s_4.L^1).\n",
    "  $\n",
    "  \n",
    "* Scale 0:\n",
    "\n",
    "  $\n",
    "    s_0.L^0 = DWT^{-1}(s_0.L^1, s_0.H^1),\\\\\n",
    "    s_4.L^0 = DWT^{-1}(s_4.L^1, s_4.H`1),\\\\\n",
    "    s_2.L^0 = MCDWT^{-1}(s_0.L^0, s_2.L^1, s_4.L^0),\\\\\n",
    "    s_1.L^0 = MCDWT^{-1}(s_0.L^0, s_1.L^1, s_2.L^0),\\\\\n",
    "    s_3.L^0 = MCDWT^{-1}(s_2.L^0, s_3.L^2, s_4.L^0).\n",
    "  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "Provided by the execution of iMCDWT.\n",
    "\n",
    "1. Scale 2:\n",
    "\n",
    "Provided by subbands `L` of the pyramids. We don't need to carry\n",
    "out any computation.\n",
    "\n",
    "2. Scale 1:\n",
    "\n",
    "Rendered after running iMCDWT one iteration. For 3 pyramids\n",
    "`A={A.L,A.H}`, `B={B.L,~B.H}` and `C={C.L,C.H}`\n",
    "where the subband `L` is the scale 2, the scale 1 is\n",
    "recostructed by (see Algorithm iMCDWT_step):\n",
    "\n",
    "```python\n",
    "  x = 2**2 = 4\n",
    "  [A.L] = 2D_iDWT(A.L,0);                              > Interpolate low-freqs A.L of V[0]\n",
    "  [A.H] = 2D_iDWT(0,A.H);                              > Interpolate high-freqs A.H of V[0]\n",
    "  V[0] = [A.L] + [A.H];                                > Reconstruct V[0] at spatial level 1\n",
    "  [B.L] = 2D_iDWT(V[1].L,0);                           > \n",
    "  [~B.H] = 2D_iDWT(0,V[1].H);\n",
    "  [C.L] = 2D_iDWT(V[2].L,0);\n",
    "  [C.H] = 2D_iDWT(0,V[2].H);\n",
    "  V[2] = [C.L] + [C.H] \n",
    "  [B.L]->[A.L] = ME([B.L], [A.L])\n",
    "  [B.L]->[C.L] = ME([B.L], [C.L])\n",
    "  [B.H]_A = MC([A.H], [B.L]->[A.L])\n",
    "  [B.H]_C = MC([C.H], [B.L]->[C.L])\n",
    "  [B.H] = [~B.H] + int(round(([B.H]_A + [B.H]_C)/2.0))\n",
    "  V[1] = [B.L] + [B.H]\n",
    "  [A.L] = [C.L]\n",
    "  [A.H] = [C.H]\n",
    "```\n",
    "\n",
    "3. Scale 0:\n",
    "\n",
    "Repeat the previous computations.\n",
    "\n",
    "4. Scale -1:\n",
    "\n",
    "Repeat the previous computations, placing 0's in the H subbands.\n",
    "\n",
    "Temporal scalability\n",
    "--------------------\n",
    "\n",
    "Provided by the pruned execution of iMCDWT. Depending on the index of\n",
    "the image to render, a number of images, that ranges between :math:`1`\n",
    "(the best case) and :math:`1+l` (the worst case), are decoded.\n",
    "\n",
    "Quality scalability\n",
    "-------------------\n",
    "\n",
    "Provided by partially reconstructing a set of coefficients selected by\n",
    "their contribution to the minimization of the distortion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sequence `S` of `N` (wavelet) pyramids, organized in `T` temporal subbands, where each temporal subband is a sequence of spatial pyramids. The number of input and output pyramids is the same.\n",
    "\n",
    "For example, if `l=2` and `n=5`:\n",
    "\n",
    "```\n",
    "      Spatial\n",
    "      scale 0 1 2       t = 1                               t = 3\n",
    "            ^ ^ ^ +---+---+-------+                   +---+---+-------+                                ^\n",
    "            | | | |   |   |       |                   |   |   |       |                                |\n",
    "            | | v +---+---+       |                   +---+---+    O <---- T[3][y][x]                  |\n",
    "            | |   |   |   |       |                   |   |   |       |                                |\n",
    "            | v   +---+---+-------+                   +---+---+-------+ l = 0                          |\n",
    "            |     |       |       |                   |       |       |                                |\n",
    "            |     |       |       |                   |       |       |                                |\n",
    "            |     |       |       |                   |       |       |                                |\n",
    "            v     +-------+-------+       t = 2       +-------+-------+                                |\n",
    "                      |       |     +---+---+-------+     |        |                                 ^ |\n",
    "                      |       |     |   |   |       |     |        |                                 | |\n",
    "                      |       +---->+---+---+       |<----+        |                                 | |\n",
    "                      |             |   |   |       |              |                                 | |\n",
    "                      |             +---+---+-------+ l = 1        |                                 | |\n",
    "                      |             |       |       |              |                                 | |\n",
    "                      |             |       |       |              |                                 | |\n",
    "                      |             |       |       |              |                                 | |\n",
    "      t = 0           |             +-------+-------+              |           t = 4                 | |\n",
    "+---+---+-------+     |                 |       |                  |     +---+---+-------+         ^ | |\n",
    "|   |   |       |     |                 |       |                  |     |   |   |       |         | | |\n",
    "+---+---+       |<----+                 |       |                  +---->+---+---+       |         | | |\n",
    "|   |   |       |                       |       |                        |   |   |       |         | | |\n",
    "+---+---+-------+                       |       |                        +---+---+-------+  l = 2  | | |\n",
    "|       |       |                       |       |                        |       |       |         | | |\n",
    "|       |       |<----------------------+       +----------------------->|       |       |         | | |\n",
    "|       |       |                                                        |       |       |         | | |\n",
    "+-------+-------+                                                        +-------+-------+         v v v\n",
    "      GOP 0                                       GOP 1                             Temporal scale 2 1 0\n",
    "<---------------><----------------------------------------------------------------------->\n",
    "\n",
    "(X --> Y) = X depends on Y (X has been encoded using Y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A implementation\n",
    "\n",
    "### Forward MCDWT\n",
    "\n",
    "```python\n",
    "n = 5 # Number of frames of the video\n",
    "l = 2 # Number of temporal scales to generate\n",
    "\n",
    "x = 2 # A constant\n",
    "for j in range(l):\n",
    "    2D_DWT(V[0]) # 1-level 2D-DWT\n",
    "    [A.L] = 2D_iDWT(V[0].L, 0)\n",
    "    [A.H] = 2D_iDWT(0, V[0].H)\n",
    "    i = 0 # Image index\n",
    "    while i < (n//x):\n",
    "        2D_DWT(V[x*i+x//2])\n",
    "        [B.L] = 2D_iDWT(V[x*i+x//2].L, 0)\n",
    "        [B.H] = 2D_iDWT(0, V[x*i+x//2].H)\n",
    "        2D_DWT(V[x*i+x])\n",
    "        [C.L] = 2D_iDWT(V[x*i+x].L, 0)\n",
    "        [C.H] = 2D_iDWT(0, V[x*i+x].H)\n",
    "        [B.L]->[A.L] = ME([B.L], [A.L])\n",
    "        [B.L]->[C.L] = ME([B.L], [C.L])\n",
    "        [B.H]_A = MC([A.H], [B.L]->[A.L])\n",
    "        [B.H]_C = MC([C.H], [B.L]->[C.L])\n",
    "        [~B.H] = [B.H] - int(round(([B.H]_A + [B.H]_C)/2.0))\n",
    "        2D_DWT([~B.H])\n",
    "        [~B.H].L = B.L\n",
    "        [A.L] = [C.L]\n",
    "        [A.H] = [C.H]\n",
    "        i += 1\n",
    "    x *= 2\n",
    "```\n",
    "\n",
    "Example (3 temporal scales (`l=2` iterations of the transform) and `n=5` images):\n",
    "```\n",
    "V[0] V[1] V[2] V[3] V[4]\n",
    " A    B    C              <- First call of MCDWT_step\n",
    "           A    B    C    <- Second call of MCDWT_step\n",
    " A         B         C    <- Third call of MCDWT_step\n",
    "---- -------------------\n",
    "GOP0        GOP1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/color_dwt.py\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pywt\n",
    "import math\n",
    "\n",
    "def _2D_DWT(image):\n",
    "    '''2D DWT of a color image.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "\n",
    "        image : [:,:,:].\n",
    "\n",
    "            A color frame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "        (L, H) where L = [:,:,:] and\n",
    "        H = (LH, HL, HH), where LH, HL, HH = [:,:,:].\n",
    "\n",
    "            A color pyramid.\n",
    "\n",
    "    '''\n",
    "\n",
    "    y = math.ceil(image.shape[0]/2)\n",
    "    x = math.ceil(image.shape[1]/2)\n",
    "    LL = np.ndarray((y, x, 3), np.float64)\n",
    "    LH = np.ndarray((y, x, 3), np.float64)\n",
    "    HL = np.ndarray((y, x, 3), np.float64)\n",
    "    HH = np.ndarray((y, x, 3), np.float64)\n",
    "    for c in range(3):\n",
    "        (LL[:,:,c], (LH[:,:,c], HL[:,:,c], HH[:,:,c])) = pywt.dwt2(image[:,:,c], 'db5', mode='per')\n",
    "\n",
    "    return (LL, (LH, HL, HH))\n",
    "\n",
    "def _2D_iDWT(L, H):\n",
    "    '''2D 1-iteration inverse DWT of a color pyramid.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "\n",
    "        L : [:,:,:].\n",
    "\n",
    "            Low-frequency color subband.\n",
    "\n",
    "        H : (LH, HL, HH), where LH ,HL, HH = [:,:,:].\n",
    "\n",
    "            High-frequency color subbands.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "        [:,:,:].\n",
    "\n",
    "            A color frame.\n",
    "\n",
    "    '''\n",
    "\n",
    "    LH = H[0]\n",
    "    HL = H[1]\n",
    "    HH = H[2]\n",
    "    frame = np.ndarray((L.shape[0]*2, L.shape[1]*2, 3), np.float64)\n",
    "    for c in range(3):\n",
    "        frame[:,:,c] = pywt.idwt2((L[:,:,c], (LH[:,:,c], HL[:,:,c], HH[:,:,c])), 'db5', mode='per')\n",
    "    return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/image_io.py\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class InputFileException(Exception):\n",
    "    pass\n",
    "\n",
    "class ImageReader:\n",
    "    '''Read 16-bit PNG images from disk.\n",
    "    \n",
    "    Images must be enumerated (image000.png, image001.png, ...).\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def read(self, number, path='./'):\n",
    "        '''Read an image from disk.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "            number : int.\n",
    "\n",
    "                Index of the image in the sequence.\n",
    "\n",
    "            path : str.\n",
    "\n",
    "                Image path.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "            [:,:,:].\n",
    "\n",
    "                A color image.\n",
    "\n",
    "        '''\n",
    "\n",
    "        file_name = '{}{:03d}.png'.format(path, number)\n",
    "        image = cv2.imread(file_name, -1)\n",
    "        if image is None:\n",
    "            raise InputFileException('{} not found'.format(file_name))\n",
    "        else:\n",
    "            image -= 32768\n",
    "            return image\n",
    "\n",
    "class ImageWritter:\n",
    "    '''Write 16-bit PNG images to disk.\n",
    "\n",
    "    Images should be enumerated (image000.png, image001.png, ...).\n",
    "\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def write(self, image, number=0, path='./'):\n",
    "        '''Write an image to disk.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "            image : [:,:,:].\n",
    "\n",
    "                The color image to write.\n",
    "\n",
    "            number : int.\n",
    "\n",
    "                Index of the image in the sequence.\n",
    "\n",
    "            path : str.\n",
    "\n",
    "                Path to the image.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "            None.\n",
    "\n",
    "        '''\n",
    "\n",
    "        file_name = '{}{:03d}.png'.format(path, number)\n",
    "\n",
    "        image += 32768\n",
    "        \n",
    "        assert (np.amax(image) < 65536), '16 bit unsigned int range overflow'\n",
    "        assert (np.amin(image) >= 0), '16 bit unsigned int range underflow'\n",
    "        \n",
    "        cv2.imwrite(file_name, np.rint(image).astype(np.uint16))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../src/motion_compensation.py\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def motion_compensation(curr, next, base):\n",
    "    flow = motion_estimation(curr, next)\n",
    "    return estimate_frame(base, flow)\n",
    "\n",
    "def motion_estimation(curr, next):\n",
    "    curr_y, _, _ = cv2.split(curr)\n",
    "    next_y, _, _ = cv2.split(next)\n",
    "\n",
    "    return cv2.calcOpticalFlowFarneback(next_y, curr_y, \n",
    "            None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "def estimate_frame(base, flow):\n",
    "    height, width = flow.shape[:2]\n",
    "    map_x = np.tile(np.arange(width), (height, 1))\n",
    "    map_y = np.swapaxes(np.tile(np.arange(height), (width, 1)), 0, 1)\n",
    "    map_xy = (flow + np.dstack((map_x, map_y))).astype('float32')\n",
    "\n",
    "    return cv2.remap(base, map_xy, None, \n",
    "            interpolation=cv2.INTER_LINEAR,\n",
    "            borderMode=cv2.BORDER_REPLICATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load ../src/mcdwt_transform.py\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pywt\n",
    "import math\n",
    "\n",
    "import image_io\n",
    "import pyramid_io\n",
    "import motion_compensation\n",
    "\n",
    "def forward(input = '../images/', output='/tmp/', n=5, l=2):\n",
    "    '''A Motion Compensated Discrete Wavelet Transform.\n",
    "\n",
    "    Compute the 1D-DWT along motion trajectories. The input video (as\n",
    "    a sequence of images) must be stored in disk (<input> directory)\n",
    "    and the output (as a sequence of DWT coefficients that are called\n",
    "    pyramids) will be stored in disk (<output> directory). So, this\n",
    "    MCDWT implementation does not transform the video on the fly.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "\n",
    "        input : str\n",
    "\n",
    "            Path where the input images are. Example:\n",
    "            \"../input/image\".\n",
    "\n",
    "        output : str\n",
    "\n",
    "            Path where the (transformed) pyramids will be. Example:\n",
    "            \"../output/pyramid\".\n",
    "\n",
    "         n : int\n",
    "\n",
    "            Number of images to process.\n",
    "\n",
    "         l : int\n",
    "\n",
    "            Number of leves of the MCDWT (temporal scales). Controls\n",
    "            the GOP size. Examples: `leves`=0 -> GOP_size = 1, `leves`=1 ->\n",
    "            GOP_size = 2, `levels`=2 -> GOP_size = 4. etc.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "        None.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    #import ipdb; ipdb.set_trace()\n",
    "    ir = image_io.ImageReader()\n",
    "    iw = image_io.ImageWritter()\n",
    "    pw = pyramid_io.PyramidWritter()\n",
    "    x = 2\n",
    "    for j in range(l): # Number of temporal scales\n",
    "        A = ir.read(0, input)\n",
    "        tmpA = _2D_DWT(A)\n",
    "        L_y = tmpA[0].shape[0]\n",
    "        L_x = tmpA[0].shape[1]\n",
    "        pw.write(tmpA, 0, output)        \n",
    "        zero_L = np.zeros(tmpA[0].shape, np.float64)\n",
    "        zero_H = (zero_L, zero_L, zero_L)\n",
    "        AL = _2D_iDWT(tmpA[0], zero_H)\n",
    "        iw.write(AL, 1)\n",
    "        AH = _2D_iDWT(zero_L, tmpA[1])\n",
    "        iw.write(AH, 1)\n",
    "        i = 0\n",
    "        while i < (n//x):\n",
    "            B = ir.read(x*i+x//2, input)\n",
    "            tmpB = _2D_DWT(B)\n",
    "            BL = _2D_iDWT(tmpB[0], zero_H)\n",
    "            BH = _2D_iDWT(zero_L, tmpB[1])\n",
    "            C = ir.read(x*i+x, input)\n",
    "            tmpC = _2D_DWT(C)\n",
    "            pw.write(tmpC, x*i+x, output)\n",
    "            CL = _2D_iDWT(tmpC[0], zero_H)\n",
    "            CH = _2D_iDWT(zero_L, tmpC[1])\n",
    "            BHA = motion.motion_compensation(BL, AL, AH)\n",
    "            BHC = motion.motion_compensation(BL, CL, CH)\n",
    "            iw.write(BH, x*i+x//2, output+'predicted')\n",
    "            prediction = (BHA + BHC) / 2\n",
    "            iw.write(prediction+128, x*i+x//2, output+'prediction')\n",
    "            rBH = BH - prediction\n",
    "            iw.write(rBH, x*i+x//2, output+'residue')\n",
    "            rBH = _2D_DWT(rBH)\n",
    "            #import ipdb; ipdb.set_trace()\n",
    "            pw.write(rBH, x*i+x//2 + 1000)\n",
    "            rBH[0][0:L_y,0:L_x,:] = tmpB[0]\n",
    "            pw.write(rBH, x*i+x//2, output)\n",
    "            AL = CL\n",
    "            AH = CH\n",
    "            i += 1\n",
    "            print('i =', i)\n",
    "        x *= 2\n",
    "\n",
    "def backward(input = '/tmp/', output='/tmp/', n=5, l=2):\n",
    "    '''A (Inverse) Motion Compensated Discrete Wavelet Transform.\n",
    "\n",
    "    iMCDWT is the inverse transform of MCDWT. Inputs a sequence of\n",
    "    pyramids and outputs a sequence of images.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "\n",
    "        input : str\n",
    "\n",
    "            Path where the input pyramids are. Example:\n",
    "            \"../input/image\".\n",
    "\n",
    "        output : str\n",
    "\n",
    "            Path where the (inversely transformed) images will\n",
    "            be. Example: \"../output/pyramid\".\n",
    "\n",
    "         n : int\n",
    "\n",
    "            Number of pyramids to process.\n",
    "\n",
    "         l : int\n",
    "\n",
    "            Number of leves of the MCDWT (temporal scales). Controls\n",
    "            the GOP size. Examples: `l`=0 -> GOP_size = 1, `l`=1 ->\n",
    "            GOP_size = 2, `l`=2 -> GOP_size = 4. etc.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "        None.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    #import ipdb; ipdb.set_trace()\n",
    "    ir = image_io.ImageReader()\n",
    "    iw = image_io.ImageWritter()\n",
    "    pr = pyramid_io.PyramidReader()\n",
    "    x = 2**l\n",
    "    for j in range(l): # Number of temporal scales\n",
    "        #import ipdb; ipdb.set_trace()\n",
    "        A = pr.read(0, input)\n",
    "        zero_L = np.zeros(A[0].shape, np.float64)\n",
    "        zero_H = (zero_L, zero_L, zero_L)\n",
    "        AL = _2D_iDWT(A[0], zero_H)\n",
    "        AH = _2D_iDWT(zero_L, A[1])\n",
    "        A = AL + AH\n",
    "        iw.write(A, 0)\n",
    "        i = 0\n",
    "        while i < (n//x):\n",
    "            B = pr.read(x*i+x//2, input)\n",
    "            BL = _2D_iDWT(B[0], zero_H)\n",
    "            rBH = _2D_iDWT(zero_L, B[1])\n",
    "            C = pr.read(x*i+x, input)\n",
    "            CL = _2D_iDWT(C[0], zero_H)\n",
    "            CH = _2D_iDWT(zero_L, C[1])\n",
    "            C = CL + CH\n",
    "            iw.write(C, x*i+x, output)\n",
    "            BHA = motion.motion_compensation(BL, AL, AH)\n",
    "            BHC = motion.motion_compensation(BL, CL, CH)\n",
    "            BH = rBH + (BHA + BHC) / 2\n",
    "            B = BL + BH\n",
    "            iw.write(B, x*i+x//2, output)\n",
    "            AL = CL\n",
    "            AH = CH\n",
    "            i += 1\n",
    "            print('i =', i)\n",
    "        x //=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-dcae8b8982c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-70dd340a9775>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, output, n, l)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mzero_H\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mzero_L\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_L\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_L\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mAL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_2D_iDWT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_H\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0miw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mAH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_2D_iDWT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero_L\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmpA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0miw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/MCDWT/src/image_io.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, image, number, path)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m32768\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m65536\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'16 bit unsigned int range overflow'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'16 bit unsigned int range underflow'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iMCDWT code\n",
    "\n",
    "```python\n",
    "n = 5 # Number of images\n",
    "l = 2 # Number of temporal scales\n",
    "\n",
    "x = 2**l\n",
    "for j in range(l):\n",
    "    [A.L] = 2D_iDWT(V[0].L, 0)\n",
    "    [A.H] = 2D_iDWT(0, V[0].H)\n",
    "    V[0] = [A.L] + [A.H]\n",
    "    i = 0 # Image index\n",
    "    while i < (n//x):\n",
    "        [B.L] = 2D_iDWT(V[x*i+x//2].L, 0)\n",
    "        [~B.H] = 2D_iDWT(0, V[x*i+x//2].H)\n",
    "        [C.L] = 2D_iDWT(V[x*i+x].L, 0)\n",
    "        [C.H] = 2D_iDWT(0, V[x*i+x].H)\n",
    "        V[x*i+x] = [C.L] + [C.H]\n",
    "        [B.L]->[A.L] = ME([B.L], [A.L])\n",
    "        [B.L]->[C.L] = ME([B.L], [C.L])\n",
    "        [B.H]_A = MC([A.H], [B.L]->[A.L])\n",
    "        [B.H]_C = MC([C.H], [B.L]->[C.L])\n",
    "        [B.H] = [~B.H] + int(round(([B.H]_A + [B.H]_C)/2.0))\n",
    "        V[x*i+x//2] = [B.L] + [B.H]\n",
    "        [A.L] = [C.L]\n",
    "        [A.H] = [C.H]\n",
    "        i += 1\n",
    "    x //= 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: $x.L^n = \\{x.L^{n+1}, x.H^{n+1}\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
